---
title: "Tarea 4 - Modelos Lineales Generalizados"
authors: "Blanca E. García Manjarrez – 118886 Mariano Villafuerte Gonzalez – 156057
  Thomas M. Rudolf - 169293 Yuneri Pérez Arellano - 199813"
output:
  pdf_document: 
    latex_engine: xelatex
  html_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gt)
library(ggplot2)
library(MASS)
library(patchwork)
library(R2OpenBUGS)
library(rjags)
library(coda)
```

Fecha de entrega: 06 de marzo 2024.

-   Blanca E. García Manjarrez -- 118886
-   Mariano Villafuerte Gonzalez -- 156057
-   Thomas M. Rudolf - 169293
-   Yuneri Pérez Arellano - 199813

1.  Spiegelhalter et al. (1995) analiza la mortalidad del escarabajo del
    trigo en la siguiente tabla, usando BUGS.

| Dosis  | #muertos | #expuestos |
|--------|----------|------------|
| $w_i$  | $y_i$    | $n_i$      |
| 1.6907 | 6        | 59         |
| 1.7242 | 13       | 60         |
| 1.7552 | 18       | 62         |
| 1.7842 | 28       | 56         |
| 1.8113 | 52       | 63         |
| 1.8369 | 53       | 59         |
| 1.8610 | 61       | 62         |
| 1.8839 | 60       | 60         |

Estos autores usaron una parametrización usual en dos parámetros de la
forma $p.i ≡ P(muerte|w_i)$, pero comparan tres funciones ligas
diferentes:

logit: $p_i = \frac{exp(α + βzi)}{1 + exp(α + βzi)}$

probit: $p_i = Φ(α + βzi)$

complementario log-log: $p_i = 1 − exp[−exp(α + βzi)]$

en donde se usa la covariada centrada $z_i = w_i − \bar{w}$ para reducir
la correlación entre la ordenada α y la pendiente β. En $OpenBUGS$ el
código para implementar este modelo es el que sigue:

```{r}
# define los valores de los datos
w_vec <- c(1.6907,  1.7242, 1.7552 ,1.7842, 1.8113, 1.8369, 1.8610, 1.8839) 
y_vec <- c(6, 13, 18, 28, 52, 53, 61, 60)
n_vec <- c(59, 60, 62, 56, 63, 59, 62, 60)

par(mfrow =c(3,2))



```

Lo que sigue al símbolo $#$ es un comentario, así que esta versión
corresponde al modelo $logit$. También $dbin$ denota la distribución
$binomial$ y $dnorm$ denota la distribución normal, donde el segundo
argumento denota la precisión, no la varianza (entonces las iniciales
normales para α y β tienen precisión 0.001, que son aproximadamente
iniciales planas (no informativas)). Hacer el análisis en $OpenBUGS$ (lo
hicimos con $JAGS$ debido al uso de MAC de agunos del equipo).

#logit

```{r}

cat("model{
for (i in 1:k){
y[i] ~ dbin(p[i],n[i])
logit(p[i]) <- alpha + beta*(w[i]-mean(w[]))
# probit(p[i]) <- alpha + beta*(w[i]-mean(w[])
# cloglog(p[i]) <- alpha + beta*(w[i]-mean(w[])
}
alpha ~ dnorm(0.0,1.0e-3)
beta ~ dnorm(0.0,1.0e-3)
} ", file="jags_model01a.txt")

##
data01 <- list("k"=length(n_vec), "y"=y_vec, "w"=w_vec, "n"=n_vec)
m1a <- jags.model(file = "jags_model01a.txt",
                   data = data01, 
                   n.chains = 20,
                   n.adapt = 1000) # burn-in
update(m1a,1000)

za <- jags.samples(m1a, c("alpha", "beta", "p"), 1000, type = c("trace"))


## data frame with results
results_df_P1 <- data.frame(N = max(length(za$alpha)), 
                            logit_alpha = mean(za$alpha), 
                            logit_beta = mean(za$beta), 
                            logit_p = mean(za$p))

results_df_P1 %>% gt()
```

##probit

```{r}

cat("model{
for (i in 1:k){
    y[i] ~ dbin(p[i],n[i])
    # logit(p[i]) <- alpha + beta*(w[i]-mean(w[]))
    probit(p[i]) <- alpha + beta*(w[i]-mean(w[]))
    # cloglog(p[i]) <- alpha + beta*(w[i]-mean(w[])
  }
  alpha ~ dnorm(0.0,1.0e-3)
  beta ~ dnorm(0.0,1.0e-3)
} ", file="jags_model01b.txt")

##
data01 <- list("k"=length(n_vec), "y"=y_vec, "w"=w_vec, "n"=n_vec)
m1b <- jags.model(file = "jags_model01b.txt",
                   data = data01, 
                   n.chains = 20,
                   n.adapt = 1000) # burn-in
update(m1b,1000)

zb <- jags.samples(m1b, c("alpha", "beta", "p"), 1000, type = c("trace"))



## add results
results_df_P1 <- results_df_P1 %>% mutate(probit_alpha = mean(zb$alpha), 
                                          probit_beta = mean(zb$beta), 
                                          probit_p = mean(zb$p))
results_df_P1 %>% gt()
```

##cloglog

```{r}
cat("model{
for (i in 1:k){
    y[i] ~ dbin(p[i],n[i])
    # logit(p[i]) <- alpha + beta*(w[i]-mean(w[]))
    # probit(p[i]) <- alpha + beta*(w[i]-mean(w[])
    cloglog(p[i]) <- alpha + beta*(w[i]-mean(w[]))
  }
  alpha ~ dnorm(0.0,1.0e-3)
  beta ~ dnorm(0.0,1.0e-3)
} ", file="jags_model01c.txt")

##
data01 <- list("k"=length(n_vec), "y"=y_vec, "w"=w_vec, "n"=n_vec)
m1c <- jags.model(file = "jags_model01c.txt",
                   data = data01, 
                   n.chains = 20,
                   n.adapt = 1000) # burn-in
update(m1c,1000)

zc <- jags.samples(m1c, c("alpha", "beta", "p"), 1000, type = c("trace"))


##
results_df_P1 <- results_df_P1 %>% mutate(cloglog_alpha = mean(zc$alpha), 
                                          cloglog_beta = mean(zc$beta),
                                          cloglog_p = mean(zc$p))
results_df_P1 %>% gt()
```

```{r}

par(mfrow=c(2,3))
hist(za$alpha,breaks = 50,prob=T)
hist(zb$alpha,breaks = 50,prob=T)
hist(zc$alpha,breaks = 50,prob=T)

hist(za$beta,breaks = 50,prob=T)
hist(zb$beta,breaks = 50,prob=T)
hist(zc$beta,breaks = 50,prob=T)
```

2.  Consideren las siguientes dos distribuciones condicionales
    completas, analizadas en el artículo de Casella y George (1992) que
    les incluí como lectura:

    $f(x|y) ∝ y e^{-yx}$ $0 < x < B < ∞$
    $f(y|x) ∝ x e^{-xy}$ $0 < y < B < ∞$

- Obtener un estimado de la distribución marginal de X cuando B = 10 usando el Gibbs
sampler.

```{r}
fxy <- function(x, y){
  return(y*exp(-y*x))
}
fyx <- function(y, x){
  return(x*exp(-x*y))
}

fx_marg <- function(x){
  return(1/x)
}

B <- 10
N <- 10000
y0 <- 1
X <- NULL
fx <- NULL
Y <- y0
for(k in 1:N){
  xi <- runif(1, 0, B)
  
  X <- append(X, xi)
  fx <- append(fx, fx_marg(xi))
  X <- append(X, xi)
}

results_df_P2a <- data.frame(x = X, y=fx)
results_df_P2a_order <- results_df_P2a[order(results_df_P2a$x),]
ggplot(results_df_P2a, aes( x=x, y = y)) + 
  geom_point()
```



- Ahora supongan que $B = ∞$ así que las distribuciones condicionales completas son
ahora las ordinarias distribuciones exponenciales no truncadas. Mostrar analíticamente
que $f_x(t) = 1/t$ es una solución a la ecuación integral en este caso:

$f_x(x)=\int{[\int f_{x|y}(x|y) f_{y|x}(y|x) dy] f_x(t)dt}$

¿El Gibbs sampler convergerá a esta solución?

3.  Supongan que una variable aleatoria y se distribuye de acuerdo a la
    densidad poli-Cauchy:
    $g(y) = \prod_i^n{\frac{1}{\pi (1+(y-a_i)^2)}}$

donde $a = (a1, . . . a_n)$ es un vector de parámetros. Supongan que
$n = 6$ y $a = (1, 2, 2, 6, 7, 8)$.

-   Escriban una función que calcule la log-densidad de y.

```{r}
logy <- function(vec_a, y){
  logg_y <- 0
  for (k in 1:length(vec_a)){
    arg_log <- pi*(1 + (y-vec_a[k])^2)
    logg_y <- logg_y - log(arg_log)
  }
  return(logg_y)
}

```


-   Escriban una función que tome una muestra de tamaño 10,000 de la
    densidad de y, usando Metropolis-Hastings con función propuesta una
    caminata aleatoria con desviación estandar C. Investiguen el efecto
    de la elección de C en la tasa de aceptación, y la mezcla de la
    cadena en la densidad.

```{r}
vec_a <- c(1,2,2,6,7,8)
n = 6
N = 10000

y <- runif(1)
f_x = logy(vec_a, y)
mu_f = 0
var_f = c(100, 10, 1, 0.1, 0.01, .001)
acceptance_count_vec <- NULL
for (m in var_f){
    acceptance_count = 0
    for (k in 1:N){
      y_new <- y + rnorm(1, mu_f, m)
      f_x_new <- logy(vec_a, y_new)
      acceptance <- min(1, y*f_x_new/(y_new*f_x))
      compar_value = runif(1)
      y_vec <- append(y_vec, ifelse(compar_value<acceptance, y_new, y))
      f_x <- ifelse(compar_value<acceptance, f_x_new, f_x)
      if(compar_value<acceptance){
        acceptance_count = acceptance_count + 1
      }
    y <- y_vec[length(y_vec)]
    }
   acceptance_count_vec <- append(acceptance_count_vec, acceptance_count)
}
results_df_P3 <- data.frame(var_vec = var_f,
                            acceptance_count_vec)

results_df_P3 %>% gt()
```


-   Usando la muestra simulada de una “buena” elección de C, aproximar
    la probabilidad P(6 \< Y \< 8).

Suponmos que una buena C es 0.1.

```{r}
vec_a <- c(1,2,2,6,7,8)
n = 6
N = 10000
for(t in 1:N/100){
  y <- runif(1)
  f_x = logy(vec_a, y)
  mu_f = 0
  var_f = c(0.1)
  acceptance_count_vec <- NULL
  P_vec <- NULL
  for (m in var_f){
      acceptance_count = 0
      for (k in 1:N){
        y_new <- y + rnorm(1, mu_f, m)
        f_x_new <- logy(vec_a, y_new)
        acceptance <- min(1, y*f_x_new/(y_new*f_x))
        compar_value = runif(1)
        y_vec <- append(y_vec, ifelse(compar_value<acceptance, y_new, y))
        f_x <- ifelse(compar_value<acceptance, f_x_new, f_x)
        if(compar_value<acceptance){
          acceptance_count = acceptance_count + 1
        }
      y <- y_vec[length(y_vec)]
      }
  }
  acceptance_count_vec <- append(acceptance_count_vec, acceptance_count)
  
  low_lim <- 6
  up_lim <- 8
  
  values_smaller_equal_than_low_lim <- results_df_P3c$Y[results_df_P3c$Y<=low_lim]
  
  values_smaller_equal_than_up_lim <- results_df_P3c$Y[results_df_P3c$Y<=up_lim]
  n_low <- length(values_smaller_equal_than_low_lim)
  n_up <- length(values_smaller_equal_than_up_lim)
  n_total <- length(results_df_P3c$Y)
  P <- (n_up-n_low)/n_total
  P_vec <- append(P_vec, P)
}
results_df_P3cP <- data.frame(P_vec, 
                              acceptance_count_vec)


```

    
4. Supongan que el vector (X, Y ) tiene función de distribución conjunta:

y deseamos simular de la densidad conjunta.

- Mostrar que la densidad condicional $f(x|y)$ es una Gamma e identificar los parámetros.
- Mostrar que la densidad condicional $f(y|x)$ es Poisson.
- Escriban una función para implementar el Gibbs sampler cuando las constantes son
dadas con valores $a = 1$ y $b = 1$.
-Con su función, escriban 1000 ciclos del Gibbs sampler y de la salida, hacer los histogramas
y estimar $E(Y )$.


5. Supongan que se tiene una matriz de 4 × 4 de variables aleatorias Bernoulli, y la denotamos por $[X_{ij} ]$, y sea $N(X)$ el número total de éxitos en $X$ (la suma de $X$) y $D(X)$ es el total de vecinos de dichos éxitos (horizontales o verticales) que difieren. Por ejemplo,

