---
title: "tarea_03_GLM_rdf"
authors: "Blanca E. García Manjarrez – 118886 Mariano Villafuerte Gonzalez – 156057
  Thomas M. Rudolf - 169293 Yuneri Pérez Arellano - 199813"
output:
  pdf_document: 
    latex_engine: xelatex
  html_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(gt)
library(ggplot2)
```

## Problema 1

Calcular el estimador de Monte Carlo de la integral

$\int_{0}^{pi/3} sin(t)\,dt$

y comparar el estimador con el valor exacto de la integral.

```{r}
## solución analitica
## integral de sin(t) es -cos(t)
sol_anlitica = -cos(pi/3) + cos(0)

## solución MCMC

num_samples = 100000
h <- function(t){sin(t)}
n_samples <- h(runif(num_samples, 0, pi/3))
result_n <- cumsum(n_samples)/(1:num_samples)
err_std <- sqrt(cumsum((n_samples-result_n)^2))/(1:num_samples)
plt_df_p1 <- data.frame(n=1:num_samples, 
                     result_n, 
                     err_std, 
                     upper = result_n + 2*err_std, 
                     lower = result_n - 2*err_std )
ggplot(plt_df_p1,  aes(x=n, y=result_n)) +
  geom_line() +
  geom_hline(yintercept = (-cos(pi/3)+cos(0))) +
  geom_line(aes(x=n, y=upper), color = "red") +
  geom_line(aes(x=n, y=lower), color = "red")

df_result_P1 <- data.frame(analitic_value = sol_anlitica, 
                           mc_value = result_n[length(result_n)])
df_result_P1 %>% gt()

```

## Problema 2

Escribir una función para calcular el estimador de Monte Carlo de la
función de distribución Be (3, 3) y usar la función para estimar F(x)
para x = 0.1, . . . , 0.9. Comparar los estimados con los valores
obtenidos con la función pbeta de R.

```{r}
a = 3
b = 3
num_sample <- 10000
n = seq(0,1, length.out= num_sample)
plt_df_p2 <- data.frame(n, 
                        beta_dist = dbeta(n, a,b))

g2 <- ggplot(plt_df_p2, aes(x=n, y= beta_dist))+
  geom_line() +
  ylab("beta(3, 3)") 

## take samples
n_samples <- rbeta(num_sample, a,b)
## calculate I_hat 
I_hat <- sum(n_samples)/length(n_samples)
## variance
var_I_hat <- sum((n_samples-I_hat)^2)/length(n_samples)
# estimator MC
x = seq(0.1,0.9, 0.1)
estim_MC_p2 <- pnorm(seq(0.1,0.9, 0.1 ),I_hat,  sqrt(var_I_hat) )
comp_pbeta <- pbeta(x, a,b)
plt_df_P2_result <- data.frame(estim_MC_p2, comp_pbeta, x)

plt_df_P2_result %>%gt()

ggplot(plt_df_P2_result)+
  geom_line(aes(x=x, y= estim_MC_p2), colour="red")+
  geom_line(aes(x=x, y=comp_pbeta), colour="blue")
  

```

##Problema 3 Usar integración Monte Carlo para estimar:

```         
$\int_0^1 \frac{e^{-x}}{1+x^2}dx$
```

y calcular el tamaño de muestra necesario para obtener un error de
estimación máximo de ±0.001

```{r}
set.seed(123)
N <- 1000
err_estim_max <- 0.001
h <- function(x){
  return(exp(-x)/(1+x^2))
}
x1 <- runif(N)
IntMC <- cumsum(h(x1))/(1:N)
sd_n <- sqrt((x1-IntMC)^2)/(1:N)
#Analytically solved o integral
sol_analytically <- integrate(h, 0, 1)$value

# find the first value that fulfills 
err_n <- IntMC - sol_analytically

plt_df_p3 <- data.frame(x = seq(1:N), 
                        y = IntMC, 
                        fcn=h(x1),
                        err_n = err_n)
ggplot(plt_df_p3) +
  geom_hline(yintercept = c(err_estim_max, -err_estim_max), colour="red") +
  geom_line(aes(x=x, y=err_n), color="blue")+
  ylim(-0.0015, 0.0015)



  
```

```{r}
err <- numeric(100)
#estimate max number of samples
set.seed(12)
num_of_sample <- 10
err_estim <- Inf
err_requ <- 0.001

while(abs(err_estim) > err_requ){
  n_samples <- runif(num_of_sample)
  IntMC_nsf <- sum(h(n_samples))/(num_of_sample)
  err_estim <- (IntMC_nsf - sol_analytically)
  num_of_sample = num_of_sample +10
}
df_result_P3 <- data.frame(num_of_sample, 
                           err_estim,
                           IntMC_nsf,
                           sol_analytically)

df_result_P3 %>%gt()
```

##Problema 4 Sea $\hat{\theta}_{IS}$ el estimador de importancia de
$\theta = \int g(x) dx$, donde la función de importancia $f$ es una
densidad. Probar que si $g(x)/f(x)$ es acotada, entonces la varianza del
estimador de muestreo por importancia $\hat{\sigma}_IS$ es finita.

##Problema 5 Encontrar dos funciones de importancia f1 y f2 que tengan
soporte en (1, ∞) y estén ‘cerca’ de:
$g(x) = \frac{x^2}{\sqrt{2\pi}} e^{\frac{-x^2}{2}}$, x \> 1

¿Cuál de las dos funciones de importancia debe producir la varianza más
pequeña para estimar la integral siguiente por muestreo de importancia?

```         
  $\int_1^\inf{\frac{x^2}{\sqrt{2\pi}}e^{\frac{-x^2}{2}}}$
```

```{r}
# 
## plot g(x)
N <- 10000
x <- runif(N, 1,5)# seq(1, 5, length.out = N)
g <- function(x){
  g <- x^2/sqrt(2*pi) * exp(-x^2 / 2)
  return(g)
}
y <- g(x)
plt_df_p5 <- data.frame(x = x,
                        y = y)

## gamma distribution has a similar form as g(x)
dist_IS1 <- dgamma(x, 5,3)#/1.7
dist_IS2 <- dnorm(x, 1.4,0.8)#/1.6
plt_df_p5 <- plt_df_p5 %>% mutate(f1 = dist_IS1,
                                  f2 = dist_IS2) 
g5 <- ggplot(plt_df_p5, aes(x=x, y=y))+
  geom_line()


g5 <- g5 + geom_line(aes(x=x, y=f1), color="red") +
  geom_line(aes(x=x, y=f2), color="green") +
    labs(title = "Function and IS Function to ensure that:\n 1. it is easy to simulate\n 2. the funcions are similar to f(theta) \n 3. has heavier tail than f(theta)\n the red one is a gamma distribution and the green one a normal distribution",
       x = "samples",
       y = "Densidad funcitons", 
       colours)
g5
  
```

```{r}
set.seed(123)
## sample of f1 and f2
n = 1000
sample_f1 <- rgamma(n,5,3)#/1.7
sample_f2 <- rnorm(n, 1.4,0.8)#/1.6

## Calculating I_hat_MI1
I_hat_MI1 <- 1/n* sum(g(sample_f1)/(pgamma(sample_f1, 5,3))) #/1.7
##calculating the varianz of f1
var_f1 <- 1/n^2 * sum(((g(sample_f1)/(pgamma(sample_f1, 5,3)))- I_hat_MI1)^2)
## Calculating I_hat_MI2
I_hat_MI2 <- 1/n* sum(g(sample_f2)/(pnorm(sample_f2, 1.4,0.8)))#/1.6
##calculating the varianz of f2
var_f2 <- 1/n^2 * sum(((g(sample_f2)/(pnorm(sample_f1, 1.4,0.8)))- I_hat_MI2)^2)
## result
result_df_p5 <- data.frame(I_hat = c(I_hat_MI1,I_hat_MI2),
                           var_I_hat = c(var_f1, var_f2))
result_df_p5 %>% gt()
```

##Problema 6 Usar el algoritmo de Metropolis-Hastings para generar
variadas aleatorias de una densidad Cauchy estándar. Descartar las
primeras 1000 observaciones de la cadena, y comparar los deciles de las
observaciones generadas con los deciles de la distribución Cauchy
estándar. Recordar que una densidad Cauchy(θ,η) tiene densidad dada por
la siguiente función:

````         
$f(x) = \frac {1}{\theta \pi (1 + [\frac{x-\nu}{\theta}]^2)}, x ∈ R, θ > 0$

La densidad Cauchy estándar tiene θ = 1, η = 0, y corresponden a la
densidad t con un grado de libertad.

some info: <https://www.youtube.com/watch?v=0lpT-yveuIA>,
<https://www.youtube.com/watch?v=yCv2N7wGDCw>,
<https://www.youtube.com/watch?v=yApmR-c_hKU&t=0s>,
<https://www.youtube.com/watch?v=prZMpThbU3E&t=0s>

```{r}
set.seed(123)
fcn_P6 <- function(x, nu, theta) {
  den_f = theta*pi*(1+((x-nu)/theta)^2) 
  f = 1/den_f
  return(f)
}
N = 10000
# 
theta <- 1
nu <- 0
x_plt <- runif(N, -10, 10)
pdf_Cauchy_estandard <- fcn_P6(x_plt, 0, 1)
plt_df_p6 <- data.frame(x,
                       y = fcn_P6(x_plt, nu, theta))
ggplot(plt_df_p6, aes(x=x, y=y))+
  geom_line()


```

Since Cauchy distribution is symmetric, one can choose a normal
distribution as $s(\theta)$ and
$\frac{Q(\theta|\theta^*)}{Q(\theta^*|\theta)}=1$
````

```{r}
# definition of normal distribution parameters: mu = 0, var = 1
mu_f <- 0
var_f <- 1
# start value:
x <- 1
#var <- 0.1
f_x <- fcn_P6(x, 0,1)#dnorm(x, mu_f, var_f)
x_vec <- c()

for(idx in 1:N){
  x_new <- x + rnorm(1, mu_f, var_f)
  f_x_new <- fcn_P6(x_new, 0,1)#dnorm(x_new, mu_f, var_f)
  acceptance <- min(1, f_x_new/f_x)
  compar_value = runif(1)
  x_vec <- append(x_vec, ifelse(compar_value<acceptance, x_new, x))
  f_x <- ifelse(compar_value<acceptance, f_x_new, f_x)
  x <- x_vec[length(x_vec)]
}

## reject the first 1000 samples
x_vec_result <- x_vec[1001:length(x_vec)]
x_vec_result_sorted <- sort(x_vec_result)
f_result_mcmc <- fcn_P6(x_vec_result_sorted, 0,1)#dnorm(x_vec_result_sorted, mu_f, var_f)

plt_df_p6_result <-  data.frame(x_mcmc = x_vec_result_sorted,
                                  f_result_mcmc = f_result_mcmc, 
                                fcn_P6_result = fcn_P6(x_vec_result_sorted, nu, theta))
ggplot(plt_df_p6_result) +
  geom_histogram(aes(x=x_mcmc)) 


# deciles
result_deciles <- data.frame(decile_perc =seq(0,1, by=0.1)*100,
                             dec_mcmc = quantile(x_vec_result, probs=seq(0,1, by=0.1)), 
                             dec_couchy = quantile(rcauchy(length(x_vec_result)), probs=seq(0,1, by=0.1)) )

result_deciles %>% gt()

  

```

7.  Implementar un muestreador de Metropolis de caminata aleatoria para
    generar muestras de una distribución estándar de Laplace:
    $f(x) = \frac{1}{2}e^{-|x|}, x ∈ R$ Para el incremento, simula una
    normal estándar. Comparar las cadenas generadas cuando la
    distribución propuesta tiene diferentes varianzas. Calcular las
    tasas de aceptación de cada cadena.

```{r}
fcn_P7 <- function(x){ return(0.5*exp(-abs(x)))
}
N <- 100000
x = seq(-10,10, length.out=N)
plt_df_P7 <- data.frame(x = x, 
                        y = fcn_P7(x))
ggplot(plt_df_P7, aes(x=x, y=y))+
  geom_line()
```

```{r}
sample_mcmc_generator <- function(incr_mu=0, incr_var=1, iter=10000){
  samples <- numeric(iter)
  accept_count <- array(0,dim=iter)
  current_sample <- 0
  for (idx in 1:iter){
    # calculate new candiate
    candidate <- current_sample +rnorm(1, incr_mu, incr_var)
    # calculate new acceptance value
    acceptance_ratio <- fcn_P7(candidate)/fcn_P7(current_sample) 
    # make decision
    if(runif(1)<acceptance_ratio){
      current_sample <- candidate
      accept_count[idx] <- 1
    }
    samples[idx] <- current_sample
  }
  df_generator <- data.frame(mcmc_samples = samples, 
                             accept_count = accept_count, 
                             used_var = incr_var)
  return(df_generator)
}
```

Evaluate different variances

```{r}
num_2_eval <- 30
var_vec <- seq(0.1, 10, length.out= num_2_eval)
accept_perc <- numeric(num_2_eval)
idx = 1
for(var_eval in var_vec){
  #print(var_eval)
  tmp_df <- sample_mcmc_generator(0, var_eval, N)
  accept_perc[idx] <- sum(tmp_df$accept_count/length(tmp_df$accept_count))
  idx <- idx+1
}
plt_df_P7_result <- data.frame( var_vec, 
                               accept_perc)
ggplot(plt_df_P7_result, aes(x=var_vec,  y=accept_perc))+
  geom_bar(stat="identity", fill="skyblue", color="black") +
  labs(title = "MH with different variances", x="variances", y="acceptance rate in %")

```

##Problema 8 Desarrollar un algoritmo de Metropolis-Hastings para
muestrear de la distribución siguiente:
\|------\|------\|------\|------\|-------\|-----\|

\| 1 \| 2 \|3 \| 4 \| 5 \|6 \|

\| 0.01 \| 0.39 \| 0.11 \| 0.18 \| 0.26 \| 0.05\|

con distribución propuesta basada en un dado honesto.

```{r}
p_dist <- c(0.01, 0.39, 0.11, 0.18, 0.26, 0.05)
x <- c(1, 2, 3, 4, 5, 6)
p_honest <- 1/6
fcn_P8 <- function(init_state, iter){
  current_state <- init_state
  mcmc_samples <- c(current_state)
  for(idx in 1:iter){
    # get a candidate
    candidate <- sample(1:6, 1)
    # calculate the acceptance
    acceptance_ratio <- min(1, p_dist[candidate]/p_dist[current_state])
    if(runif(1)<acceptance_ratio){
      current_state <- candidate
    }
    mcmc_samples <- append(mcmc_samples, current_state)
  }
  return(mcmc_samples)
}
```

Evaluate

```{r}
sample_P8 <- fcn_P8(1, 1000)
p_mcmc <- c(length(which(sample_P8==1)), 
            length(which(sample_P8==2)), 
            length(which(sample_P8==3)), 
            length(which(sample_P8==4)),
            length(which(sample_P8==5)), 
            length(which(sample_P8==6)))/1000
df_result_P8 <- data.frame(dice_num = seq(1,6),
                           p_dist, 
                           p_mcmc)
df_result_P8 %>% gt()
```

9.  La sucesión de Fibonacci 1, 1, 2, 3, 5, 8, 13, . . . es descrita por
    la recurrencia $f_n = f_{n−1} + f_{n−2}$,para n ≥ 3 con
    $f_1 = f_2 = 1$

```{r}
fibonacci <- function(n){
  ## n: max number of fibonacci chain
  fibo <- numeric(n)
  fibo[1] <- 1
  fibo[2] <- 1
  for(k in 3:n){
    fibo[k] <- fibo[k-2] + fibo[k-1]
  }
  df <- data.frame(k = 1:n,
                   fibo = fibo)
  return(df)
}
test_fibo <- fibonacci(15)
test_fibo
```

a.  Mostrar que el número de sucesiones binarias de longitud m sin 1’s
    adyaentes es $f_{m+2}$

    comment:
    <https://math.stackexchange.com/questions/3172492/the-relationship-between-fibonacci-numbers-and-binary-numbers-that-have-no-conse>
    $\\ r^2 - r - 1 = 0$ $r_{1,2} = \frac{1\pm\sqrt{5}}{2}$
    $\\f_0 = c_1 r_1^0 + c_2 r_2^0 = 0$
    $\\f_1 = c_1 r_1^1 + c_2 r_2^1 = 1\\$ resolving this linear problem
    leads to: $\\ c_1 = \frac{1}{\sqrt{5}}$ $c_2 =-\frac{1}{\sqrt{5}}$

number of sequences no consecutive 1´s with $n$ number of bits:
$$a(n)=f(n+2)= \frac{1}{\sqrt{5}}(\frac{1+\sqrt5}{2})^{(n+2)}-\frac{1}{\sqrt{5}}(\frac{1-\sqrt5}{2})^{(n+2)}$$

```{r}
binary_sequ <- function(n){
  # n: number of bits in the sequence
  a <- 1/sqrt(5)
  b <- (1+sqrt(5))/2
  c <- (1-sqrt(5))/2
  num_of_ones <- a*b^(n+2)-a*c^(n+2)
}
test_fibo
bin_seq <- binary_sequ(test_fibo$k)
bin_seq <- c(1, 1, bin_seq[1:(length(bin_seq)-2)])
test_fibo <- test_fibo %>% mutate(bin_seq)
test_fibo %>% gt()
```

b.  Sea $p_{k,m}$ el número de buenas sucesiones de longitud $m$ con
    exactamente k 1’s. Mostrar que
    $$\\ p_{k,m} = \begin{pmatrix} m-k+1 \\ k \end{pmatrix}$$, k = 0,
    1,..., ceiling(m/2)

$$
p_{k.m}=\frac{(m-k+1)!}{k!(m-2k+1)!}=\frac{(m-2k+1)!*\prod_{i=0}^{k-1}(m+1-k-i)}{k!(m-2k+1)!}=\prod_{i=0}^{k-1}\frac{(m+1-k-i)}{k!}
$$

```{r}
p_km <- function(k, m){
  num <- factorial(m-k+1)
  den <- factorial(k)*factorial(m-2*k+1)
  return(num/den)
}
p_km_verif <- function(k, m){
  den=factorial(k)
  num = 1
  for(idx in seq(0, k-1)){
    num <- num*(m+1-k-idx)
  }
  return(num/den)
}

test_p <- p_km(2, 8)
test_p_verif <- p_km(4, 8)
test_p
test_p_verif
```

c.  Sea $µ_m$ el número esperados de 1’s en una buena sucesión de
    longitud m bajo la distribución uniforme. Encontrar $µ_m$ para m =
    10, 100, 1000

```{r}

```
