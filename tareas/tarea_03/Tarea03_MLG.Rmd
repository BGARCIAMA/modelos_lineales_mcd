---
title: "Tarea 3 - Modelos Lineales Generalizados"
authors: "Blanca E. García Manjarrez – 118886 Mariano Villafuerte Gonzalez – 156057
  Thomas M. Rudolf - 169293 Yuneri Pérez Arellano - 199813"
output:
  pdf_document: default
  html_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gt)
library(ggplot2)
library(MASS)
library(patchwork)
```

![](ITAM.png)

Fecha de entrega: 21 de febrero 2024.

-   Blanca E. García Manjarrez -- 118886
-   Mariano Villafuerte Gonzalez -- 156057
-   Thomas M. Rudolf - 169293
-   Yuneri Pérez Arellano - 199813

1.  Calcular el estimador de Monte Carlo de la integral
    $$\int_0^\frac{\pi}{3} \mathrm{sin}{(t)}\,\mathrm{d}t$$ y comparar
    el estimador con el valor exacto de la integral.

> El estimador de Monte Carlo es
> $$(b-a)\int_{a}^{b} g(x) \cdot dx = \frac{\pi}{3}\left\lbrace\frac{1}{m}\sum_{i=1}^{m} \mathrm{sin}{(u)}\right\rbrace$$
> donde $u$ es generada a partir de una distribución
> $U(0,\frac{\pi}{3})$

```{r}
set.seed(123) 
f_seno <- function(t) sin(t)
a <- 0
b <- (pi/3)

n <- 100000
y <- replicate(1000, expr = {
  x <- runif(n, 0, pi/3)
  theta.hat <- pi/3 * mean(f_seno(x))
  })

integral <- integrate(f_seno, a, b)$value

compara <- data.frame(Integral = integral, Est_MonteCarlo = mean(y))
compara |> gt() |> fmt_number(decimals = 6)
```

2.  Escribir una función para calcular el estimador de Monte Carlo de la
    función de distribución $Beta(3,3)$ y usar la función para estimar
    $F(x)$ para $x = 0.1, . . . , 0.9$. Comparar los estimados con los
    valores obtenidos con la función 'pbeta' de 'R'.

> Derivado de la relación matemática que existe entre la distribución
> $Gamma$ y la $Beta$, se puede llegar a que la proporción de dos
> variables $Gamma$ independientes sigue una distribución $Beta$. Este
> enfoque permite utilizar propiedades conocidas de las distribuciones
> $Gamma$ para generar muestras eficientemente y estimar la distribución
> $Beta$. Con base en lo anterior, se tiene que: $$u \sim Gamma(a,1)$$
> $$v \sim Gamma(b,1)$$ entonces la variable aleatoria
> $$Y = \frac{u}{u+v} \sim Beta(a,b)$$

```{r}
set.seed(123)
mcBeta <- function(x, a, b, n = 10000) {
  u <- rgamma(n, a, 1)
  v <- rgamma(n, b, 1)
  mean(u / (u + v) <= x)
}

x <- seq(0.1, 0.9, 0.1)
p <- sapply(x, mcBeta, a = 3, b = 3)

resultados <- data.frame(
  x = x,
  pBetaR = pbeta(x, 3, 3),
  mcBeta_est = p
)
resultados |> gt() |> fmt_number(decimals = 4)

```

3.  Usar integración Monte Carlo para estimar:
    $$\int_0^1 \frac{e^{-x}}{1+x^2}dx$$ y calcular el tamaño de muestra
    necesario para obtener un error de estimación máximo de $\pm0.001$

```{r warning=FALSE}
set.seed(123)
n <- 10000
u <- runif(n)

g <- Vectorize(function(x) exp(-x)/(1+x^2))
y <- g(u)
mc3 <- mean(y)
integral3 <- integrate(g, 0, 1)$value

# Definimos el error de estimación máximo como:
error_n <- numeric(n) 
for(i in 1:n){ 
  error_n[i] <- mean(y[1:i]) - integral3 
}

resultados3 <- data.frame(error = error_n)
ggplot(resultados3, aes(x = seq_along(error), y = error)) +
  geom_line() +
  ylim(-0.0015, 0.0015) +
  geom_hline(yintercept = c(-0.001, 0.001), linetype = "dashed", color = "red") +
  theme_minimal() +
  labs(y = "Error", x = "Tamaño de muestra") +
  theme(panel.grid = element_line(color = 'steelblue',
                                  linetype = 2))
```

```{r}
set.seed(123)
# Función a integrar
f <- function(x) exp(-x) / (1 + x^2)
integral_exacta <- integrate(f, 0, 1)$value
error_maximo <- 0.001

# Inicializar variables
n_muestra <- 10
error_estimado <- Inf

while (error_estimado > error_maximo) {
  muestras <- runif(n_muestra)
  estimacion_monte_carlo <- mean(f(muestras))
  error_estimado <- abs(integral_exacta - estimacion_monte_carlo)
  n_muestra <- n_muestra + 10
}

cat("Tamaño de muestra necesario:", n_muestra, "\n")
cat("Estimación de la integral:", estimacion_monte_carlo, "\n")
cat("Error de estimación:", error_estimado, "\n")

```

4.  Sea $\hat\theta_{IS}$ el estimador de importancia de
    $$\theta = \int g(x)dx$$ donde la función de importancia $f$ es una
    densidad. Probar que si $g(x)/f(x)$ es acotada, entonces la varianza
    del estimador de muestreo por importancia $\hat \sigma_{IS}$ es
    finita.

> Suponga que $f$ es una función de densidad,
> $$\theta=\int g(x)dx < \infty,$$ y
> $$\left|\frac{g(x)}{f(x)}\right| \leq M < \infty.$$ Sea
> $\hat\theta=\hat\theta_{IS}$ entonces:
> $$Var(\hat{\theta})=E[\hat{\theta^2}]-(E[\hat{\theta}])^2$$
> $$=E\left[\frac{1}{m}\sum_{i=1}^{m}\left(\frac{g(X_i)}{f(X_i)}\right)^2f(X_i)\right]-\theta^2$$
> $$\int \frac{g(x)^2}{f(x)}dx-\theta^2$$
> $$=\int \frac{g(x)}{f(x)}g(x)dx-\theta^2 \le M \int g(x)dx -\theta^2$$
> $$=M \theta-\theta^2 < \infty$$

5.  Encontrar dos funciones de importancia $f_1$ y $f_2$ que tengan
    soporte en $(1, \infty)$ y estén "cerca" de:
    $$g(x)= \frac{x^2}{\sqrt{2\pi}} e^{-{x^2}/2}, \qquad x<1$$ ¿Cuál de
    las dos funciones de importancia debe producir la varianza más
    pequeña para estimar la integral siguiente por muestreo de
    importancia?
    $$\int_{1}^{\infty} \frac{x^2}{\sqrt{2\pi}} e^{-{x^2}/2} \,dx$$
    \>Graficamos $g(x)$

```{r}
x <- seq(1, 10, 0.01)
y <- x^2 * exp(-x^2/2)/sqrt(2 * pi)
 
df <- data.frame(x = x, y = y)
 
ggplot(df, aes(x, y)) +
  geom_line(linetype = "solid", color = "steelblue1", linewidth = 1.5) +
  geom_line(aes(y = 2 * dnorm(x, 1)), linetype = "dashed", color = "steelblue3", linewidth = 1.5) +
  geom_line(aes(y = dgamma(x - 1, 3/2, 2)), linetype = "dotted",  color = "steelblue4", linewidth = 1.5) +
  ylim(0, 1) +
  labs(title = "Funciones g(x), f1, y f2")  +
  theme(legend.position = "top") +
  scale_linetype_manual(values = c("solid", "dashed", "dotted"),
                        name = "Leyenda",
                        labels = c("g(x)", "f1", "f2"))
```

> $f_1$ es un variable $\chi(1)$ con $x>1$ por lo que $f_1$ es le doble
> de la densidad de la $N(1,1)$. Y $f_2$ considera una $Gamma$ con
> $x>1$. Ambas $f_1$ y $f_2$ satisfacen que el soporte este entre
> $(1, \infty)$.

> Por otro lado, como podemos observar en la gráfica podemos considerar
> que la función de importancia Normal produzca la varianza más pequeña
> al estimar la integral, porque la relación $\frac{g(x)}{f(x)}$ está
> más cerca de una función constante.

```{r}
x <- seq(1, 10, 0.01)
y <- x^2 * exp(-x^2/2)/sqrt(2 * pi)
 
df <- data.frame(x = x, 
                 f1 = y/(dgamma(x - 1, 3/2, 2)), 
                 f2 = y/(2 * dnorm(x, 1)))
 
ggplot(df, aes(x)) +
  geom_line(aes(y = f1), linetype = "dotted", linewidth = 1.5, color = "steelblue1") +
  geom_line(aes(y = f2), linetype = "dashed", linewidth = 1.5, color = "steelblue4") +
  labs(title = "Funciones f1 y f2") +
theme(legend.position = "right",  
        plot.margin = margin(r = 5)) +
  scale_linetype_manual(values = c("dotted", "dashed"),
                        name = "Funciones",
                        labels = c("f1", "f2")) +
  guides(linetype = guide_legend(title = "Leyenda"))
 
```

6.Usar el algoritmo de Metropolis-Hastings para generar variadas
aleatorias de una densidad Cauchy estándar. Descartar las primeras 1000
observaciones de la cadena, y comparar los deciles de las observaciones
generadas con los deciles de la distribución Cauchy estándar. Recordar
que una densidad $Cauchy(\theta,\eta)$ tiene densidad dada por la
siguiente función: $$f(x)=\frac{1}{\theta\pi(1+[(x-\eta)/\theta]^2)},$$
$$x\epsilon\mathbb{R},\theta>0$$ La densidad Cauchy estándar tiene
$\theta$ = 1, $\eta$ = 0, y corresponden a la densidad t con un grado de
libertad.

> La siguiente cadena usa la distribución propuesta de
> $N(\mu_t,\sigma^2)$, donde $\mu_t=X_t$ es el valor previo de la
> cadena. Entonces: $$r(x_t,y)=\frac{f(y)g(x_t|y)}{f(x_t)g(y|x_t)}$$
> $$\frac{(1+x_t^2)\pi\sqrt{2\pi}\sigma e^{-(x_t-y)^2/(2\sigma^2)}}{(1+y^2)\pi \sqrt{2\pi}\sigma e^{-(y-x_t)^2/(2\sigma^2)}}$$
> $$=\frac{1+x_t^2}{1+y^2}$$

```{r}
n <- 10000
sigma <- 3
x <- numeric(n)

x[1] <- rnorm(1, 0, sigma)
k <- 0
u <- runif(n)

for (i in 2:n) {
  xt <- x[i - 1]
  y <- rnorm(1, xt, sigma)
  num <- 1 + xt^2
  den <- 1 + y^2
  num <- num * dnorm(xt, y)
  den <- den * dnorm(y, xt)
  if (u[i] <= num/den)
    x[i] <- y
  else {
    x[i] <- xt
    k <- k + 1}
  }

kdf <- data.frame(Rechazos = k)
kdf |> gt() |> fmt_number(decimals = 0, sep_mark = ",")
```

```{r}
# Cuantiles y comparación con la distribución cauchy
p <- seq(0.1, 0.9, 0.1)
burn <- 1000
xb <- x[(burn + 1):n]
Q <- quantile(xb, p)
QCau <- qcauchy(p)

resultados6 <- data.frame(p = p, Cuantiles_Est = Q, Cuantiles_Cauchy = QCau)
resultados6 |> gt() |> fmt_number(decimals = 3)
```

```{r}
p62 <- seq(0.95, 1, 0.01)
Q62 <- quantile(xb, p62)

resultados62 <- data.frame(p = p62, Cuantiles_Est = Q62, Cuantiles_Cauchy = qcauchy(p62))
resultados62 |> gt() |> fmt_number(decimals = 3)
```

7.  Implementar un muestreador de Metrópolis de caminata aleatoria para
    generar muestras de una distribución estándar de Laplace:
    $$f(x)= \frac{1}{2} e^{-\|x\|}, \\qquad x \in \mathbb{R}$$Para el
    incremento, simula una normal estándar. Comparar las cadenas
    generadas cuando la distribución propuesta tiene diferentes
    varianzas. Calcular las tasas de aceptación de cada cadena. Sea la
    distribución estándar de Laplace $$f(x)= \frac{1}{2} e^{-|x|}$$ y
    $$r(x_t,y)=\frac{f(y)}{f(x_t)}=\frac{e^{-|y|}}{e^{-|x_t|}}=e^{|x_t|-|y|} $$

```{r}
rw.Laplace <- function(N, x0, sigma) {
  # N es el largo de la cadena
  # x0 es valor inicial
  # sigma es desv. est. de la normal propuesta
  x <- numeric(N)
  x[1] <- x0 
  u <- runif(N)
  k<-0
#A cada paso, el punto propuesto se genera con una normal  #(xt, sigma^2)
  for (i in 2:N) {
    xt <- x[i - 1] # valor anterior en la cadena
    y <- rnorm(1, xt, sigma)
    if (u[i] <= exp(abs(xt) - abs(y)))
      x[i] <- y
    else {
      x[i] <- x[i - 1]
      k<-k+1
    }
  }
  return(list(x=x, k= k)) # El valor de retorno es una lista que contiene la cadena generada x y el número de puntos rechazados k
}
 
N <- 5000
sigma <- c(0.5, 1, 2, 4) 
x0 <- rnorm(1)
rw1 <- rw.Laplace(N, x0, sigma[1])
rw2 <- rw.Laplace(N, x0, sigma[2])
rw3 <- rw.Laplace(N, x0, sigma[3])
rw4 <- rw.Laplace(N, x0, sigma[4])

resultados7 <- data.frame("sigma(0.5)" = rw1$k, 
                   "sigma(1)" = rw2$k, 
                   "sigma(2)" = rw3$k, 
                   "sigma(4)" = rw4$k)

resultados7 |> gt() |> 
  tab_header(
    title = "Cadenas Generadas",
    subtitle = "Valores de Sigma") |>
  fmt_number(decimals = 0, sep_mark = ",")
```

```{r}
resultados72 <- data.frame("sigma(0.5)" = rw1$k/N, 
                   "sigma(1)" = rw2$k/N, 
                   "sigma(2)" = rw3$k/N, 
                   "sigma(4)" = rw4$k/N)
resultados72 |> gt() |> 
  tab_header(
    title = "Tasa de aceptación",
    subtitle = "Valores de Sigma") |>
  fmt_percent(decimals = 2, sep_mark = ",")
```

```{r}
b <- 100
y1 <- rw1$x[(b + 1):N]
y2 <- rw2$x[(b + 1):N]
y3 <- rw3$x[(b + 1):N]
y4 <- rw4$x[(b + 1):N]

data <- data.frame(
  Iteracion = 1:N,
  rw1 = rw1$x,
  rw2 = rw2$x,
  rw3 = rw3$x,
  rw4 = rw4$x)

par(mfrow = c(2,2))

c1 <- ggplot(data, aes(x = Iteracion)) +
  geom_line(aes(y = rw1), linetype = "solid", color = "turquoise4") +
  ggtitle("Cadena 1")
c2 <- ggplot(data, aes(x = Iteracion)) +
  geom_line(aes(y = rw2), linetype = "solid", color = "maroon3") +
  ggtitle("Cadena 2")
c3 <- ggplot(data, aes(x = Iteracion)) +
  geom_line(aes(y = rw3), linetype = "solid", color = "slateblue3") +
  ggtitle("Cadena 3")
c4 <- ggplot(data, aes(x = Iteracion)) +
  geom_line(aes(y = rw4), linetype = "solid", color = "mediumpurple") +
  ggtitle("Cadena 4")
c1 + c2 + c3 + c4
```

```{r}
par(mfrow = c(2,2))
p <- ppoints(200)
y <- qexp(p, 1)
z <- c(-rev(y), y)
fx <- 0.5 * exp(-abs(z))
hist(y1, breaks = "Scott", freq = FALSE, ylim = c(0,+ 0.5))
lines(z, fx)
hist(y2, breaks = "Scott", freq = FALSE, ylim = c(0,+ 0.5))
lines(z, fx)
hist(y3, breaks = "Scott", freq = FALSE, ylim = c(0,+ 0.5))
lines(z, fx)
hist(y4, breaks = "Scott", freq = FALSE, ylim = c(0,+ 0.5))
lines(z, fx)
par(mfrow = c(1, 1))
```

```{r}
p <- seq(0.01, 0.99, by = 0.01)

set.seed(123)
y1 <- rexp(200, rate = 1)
y2 <- rexp(200, rate = 1)
y3 <- rexp(200, rate = 1)
y4 <- rexp(200, rate = 1)

data <- data.frame(value = c(y1, y2, y3, y4),
                   group = rep(c("y1", "y2", "y3", "y4"), each = 200))

ggplot(data, aes(sample = value)) +
  geom_qq(distribution = qexp, dparams = list(rate = 1), size = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "mediumpurple") +
  facet_wrap(~group, scales = "free") +
  labs(title = "QQ Plots") 

```

> Según las gráficas anteriores, se descarta de cada cadena una pequeña
> muestra preliminar de tamaño 100. Cada una de las cadenas parece haber
> convergido hacia la distribución de Laplace objetivo. Las cadenas 2 y
> 3 correspondientes a $\sigma=1,2$ tienen los mejores ajustes según los
> gráficos qqplot. La segunda cadena es la más eficiente de estas dos.

8.  Desarrollar un algoritmo de Metropolis-Hastings para muestrear de la
    distribución siguiente:

    |      |      |      |      |      |      |
    |------|------|------|------|------|------|
    | 1    | 2    | 3    | 4    | 5    | 6    |
    | 0.01 | 0.39 | 0.11 | 0.18 | 0.26 | 0.05 |

    con distribución propuesta basada en un dado honesto.

```{r}
n <- 1000 
x <- numeric(n) 
p_acep <- 0 #Numero de pasos aceptados
p_rech <- 0 #Numero de pasos rechazados

### MCMC ###
pi <- c(0.01,0.39,0.11,0.18,0.26,0.05) # Distribución objetivo

#Generamos el primer valor de la cadena
x[1] <- sample(c(1:6),1) 

for(i in 2:n){
  actual <- x[i-1] 
  propuesta <- sample(c(1:6), 1) #pi_i
  A <- pi[propuesta]/pi[actual] #a(i,j)
  if(runif(1) <= A){ #si aceptas, te mueves a x_j 
    #y pasos aceptados incrementa 1
    x[i] <- propuesta
    p_acep <- p_acep + 1
  }
  else{ #Si es rechazado, permanecemos en x_i y pasos rechazados incrementa 1
    x[i] <- actual
    p_rech <- p_rech + 1
  }
}
```

9.  La sucesión de Fibonacci $1, 1, 2, 3, 5, 8, 13, . . .$ es descrita
    por la recurrencia $f_n = f_{n−1} + f_{n−2}$, para $n ≥ 3$ con
    $f_1 = f_2 = 1$

<!-- -->

a.  Mostrar que el número de sucesiones binarias de longitud $m$ sin 1's
    adyacentes es $f_{m+2}$

```{r}
fibonacci <- function(n){
  ## n: Maximo numero de fibonacci
  fibo <- numeric(n)
  fibo[1] <- 1
  fibo[2] <- 1
  for(k in 3:n){
    fibo[k] <- fibo[k-2] + fibo[k-1]
  }
  df <- data.frame(k = 1:n,
                   fibo = fibo)
  return(df)
}
test_fibo <- fibonacci(12)
```

> Sea $$r^2 - r - 1 = 0$$ $$r_{1,2} = \frac{1\pm\sqrt{5}}{2}$$
> $$f_0 = c_1 r_1^0 + c_2 r_2^0 = 0$$ $f_1 = c_1 r_1^1 + c_2 r_2^1 = 1$
> resolviendo este problema linear lleva a: $$c_1 = \frac{1}{\sqrt{5}}$$
> y $$c_2 =-\frac{1}{\sqrt{5}}$$ número de secuencias de 1's no
> adyacentes con $n$ número de bits:
> $$a(n)=f(n+2)= \frac{1}{\sqrt{5}}\left(\frac{1+\sqrt5}{2}\right)^{(n+2)}-\frac{1}{\sqrt{5}}\left(\frac{1-\sqrt5}{2}\right)^{(n+2)}$$

```{r}
seq_bin <- function(n){
  # n: Numero de bits en la secuencia
  a <- 1/sqrt(5)
  b <- (1+sqrt(5))/2
  c <- (1-sqrt(5))/2
  num_ones <- a*b^(n+2)-a*c^(n+2)
}

bin_seq <- seq_bin(test_fibo$k)
bin_seq <- c(1, 1, bin_seq[1:(length(bin_seq)-2)])
test_fibo <- test_fibo |> mutate(bin_seq)
test_fibo |> gt()
```

> Definimos $B_m$ como el número de sucesiones binarias de longitud $m$
sin 1's adyacentes. Analicemos los casos base:
$B_1 = 2$ (las posibles sucesiones son 0 y 1).
$B_2 = 3$ (las posibles sucesiones son 00, 01, 10).
Ahora, para $m \geq 3$, observamos que el último dígito puede ser 0 o 1.
-   Si es 0, entonces los $m-1$ dígitos anteriores formarán una sucesión
    sin 1's adyacentes, es decir, $B_{m-1}$.
-   Si el último dígito es 1, entonces el penúltimo dígito debe ser 0, y
    los $m-2$ dígitos anteriores formarán una sucesión sin 1's
    adyacentes, es decir, $B_{m-2}$. Por lo tanto, para $m \geq 3$, la
    relación de recurrencia es:
$$B_m=B_{m-1}+B_{m-2}$$
Esto coincide con la recurrencia de la sucesión de Fibonacci. Además,
dado que $B_1 = 2$ y $B_2 = 3$, podemos concluir que $B_m$ es igual al
$(m+2)$-ésimo término de la sucesión de Fibonacci, es decir,
$B_m = f_{m+2}$.

b.  Sea $p_{k,m}$ el número de buenas sucesiones de longitud $m$ con
    exactamente $k$ 1's. Mostrar que
    $$p_{k,m}=\binom{m-k+1}{k}, k=0,1, ..., ceiling(m/2)$$

    **FALTA**

c.  Sea $\mu_m$ el número esperados de 1's en una buena sucesión de
    longitud $m$ bajo la distribución uniforme. Encontrar $\mu_m$ para
    $m = 10, 100, 1000$.

    ```{r}
    # Función para calcular el número de Fibonacci
    fibonacci <- function(n) {
      if (n <= 2) {
        return(1)
      } else {
        fib_seq <- numeric(n)
        fib_seq[1:2] <- 1
        for (i in 3:n) {
          fib_seq[i] <- fib_seq[i-1] + fib_seq[i-2]
        }
        return(fib_seq)
      }
    }

    # Función para calcular mu_m
    calculate_mu <- function(m) {
      fib_sequence <- fibonacci(m + 2)
      mu_m <- m / 2 * fib_sequence[m + 2]
      return(mu_m)
    }

    # Calcular mu_m para m = 10, 100, 1000
    mu_10 <- calculate_mu(10)
    mu_100 <- calculate_mu(100)
    mu_1000 <- calculate_mu(1000)

    # Imprimir resultados
    cat("mu_10 =", mu_10, "\n")
    cat("mu_100 =", mu_100, "\n")
    cat("mu_1000 =", mu_1000, "\n")

    ```
